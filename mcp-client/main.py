from contextlib import asynccontextmanager
import google.generativeai as genai
from fastapi import FastAPI
from mcp import ClientSession
from mcp.client.sse import sse_client
from pydantic import BaseModel
import uvicorn
import os
from dotenv import load_dotenv
from google import genai
from google.genai import types

load_dotenv()

MCP_SERVER_URL = "http://127.0.0.1:8081/sse"  


gemini_client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
context_mcp_client = {}

app_state = {
    "mcp_client": None,
    "gemini_client": None
}

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifecycle manager for the FastAPI application"""
    print("Starting up MCP Client...")
    
    app_state["gemini_client"] = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
    
    async with sse_client(url=MCP_SERVER_URL) as client:
        app_state["mcp_client"] = client
        print("MCP Client initialized")
        yield
        
    
app = FastAPI(title="MCP Client", lifespan=lifespan)





@app.get("/")
async def root():
    async with sse_client(url=MCP_SERVER_URL) as client:
        async with ClientSession(*client) as session:
            await session.initialize()
            response_tools = await session.list_tools()
            tools = response_tools.tools
            print(f"Connected to server with tools: {[tool.name for tool in tools]}")
            return {"message": "Welcome to the MCP Client! I am connected to an MCP server with these tools", "tools": [tool.name for tool in tools]}


class QueryRequest(BaseModel):
    query: str


#TODO [FIXME] avoid instantiating the client every time a request is made

@app.post("/process/")
async def process_query(query: QueryRequest):
    print(f"Processing query: {query}")
    async with sse_client(url=MCP_SERVER_URL) as client:
        async with ClientSession(*client) as session:
            await session.initialize()
            response = await process_query(session, query.query)
            return {"response": response}

async def process_query(session, query: str) -> str:
    """
    Process a query using Gemini and available tools
    """
    print(f"Processing query: {query}")
    print("_" * 30)
    mcp_tools = await session.list_tools()
    print(f"mcp_tools: {mcp_tools}")
    print("_" * 30)
    tools = [
        types.Tool(
            function_declarations=[
                {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": {
                        k: v
                        for k, v in tool.inputSchema.items()
                        if k not in ["additionalProperties", "$schema"]
                    },
                }
            ]
        )
        for tool in mcp_tools.tools
    ]
    print(f"Available tools: {[tool for tool in tools]}")
    print("_" * 30)


    response = app_state["gemini_client"].models.generate_content(
        model="gemini-2.0-flash",
        contents=query,
        config=types.GenerateContentConfig(
            temperature=0,
            tools=tools,
        ),
    )
    print(f"Response: {response}")
    print("_" * 30)
    try:
        candidates = response.candidates
        print(f"Candidates: {candidates}")
        print("_" * 30)
        first_candidate = candidates[0]
        print(f"First candidate: {first_candidate}")
        print("_" * 30)

        first_candidate_part = first_candidate.content.parts[0]
        print(f"First candidate part: {first_candidate_part}")
        print("_" * 30)    
    except Exception as e:
        print(f"Error: {e}")
        return "Error processing the query."
    

    if len(response.candidates) > 0 and response.candidates[0].content.parts[0].function_call:
        function_call = response.candidates[0].content.parts[0].function_call

        result = await session.call_tool(
            function_call.name, arguments=dict(function_call.args)
        )
        print(f"Function call result: {result}")
        return result.content[0].text
    else:
        print("No function call was generated by the model.")
        if response.text:
            print("Model response:")
            print(response.text)
            return response.text
        else:
            print("No text response from the model.")
            return "No text response from the model."

if __name__ == "__main__":

    uvicorn.run(app, host="127.0.0.1", port=8082)